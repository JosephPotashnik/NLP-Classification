{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babylon Task - Joseph Potashnik\n",
    "\n",
    "## Thought process\n",
    "\n",
    "1. Loading dataset.\n",
    "2. Independent variables: text and agent (Doctor / Patient).\n",
    "3. Dependent variables: coarse-grained, fine-grained tag.\n",
    "4. We will build a Pipeline that combines the text and speaker identity (via FeatureUnion), then uses a classifier (my choice was SVM, subject to future experimentation).\n",
    "5. Hyperparameters tuning via GridSearchCV. The hyperparameter space is vast; here I shall restrict myself to the relative weights of agent / text features in the classification.\n",
    "6. Results! and analysis.\n",
    "\n",
    "## text pipeline\n",
    "1. selecting the 'text' column from the train dataframe.\n",
    "2. using a bag-of-words (with ngram=1) model with CountVectorizer on each senetence, we also use spaCy tokenizer that strips punctation and stop words.\n",
    "\n",
    "## agent pipeline\n",
    "1. selecting the 'agent_type' column from the train dataframe.\n",
    "2. converting to int with regular CountVectorizer. Assumption on input: we only encounter the category word (e.g, 'Patient', 'Doctor').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports\n",
    "\n",
    "import libraries and read files into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy as spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df_train = pd.read_csv('interactions_train.tsv', sep='\\t', header=0, index_col=0)\n",
    "df_test = pd.read_csv('interactions_test.tsv', sep='\\t', header=0, index_col=0)\n",
    "\n",
    "def load_train_test_sets(independentvars, dependentVars):\n",
    "    X_train = df_train[independentvars]\n",
    "    y_train = df_train[dependentVars]\n",
    "    X_test = df_test[independentvars]\n",
    "    y_test = df_test[dependentVars]\n",
    "    return (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens = nlp(sentence)\n",
    "    tokens = [tok for tok in tokens if not (tok.is_stop or tok.is_punct)]   \n",
    "    # spaCy's behavior returns PRON as the lemma of pronouns. \n",
    "    #in my mind, it's wrong and subject to future change.\n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the pipeline for coarse classification: depends on agent and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelines_for_text_and_agent():\n",
    "    pipe_for_text = Pipeline([\n",
    "                    ('selector', TextSelector(key='text')),\n",
    "                    ('cv', CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)))\n",
    "                ])\n",
    "\n",
    "    pipe_for_agent = Pipeline([\n",
    "                    ('selector', TextSelector(key='agent_type')),\n",
    "                    ('cv', CountVectorizer())\n",
    "                ])\n",
    "    return pipe_for_text, pipe_for_agent\n",
    "\n",
    "def pipeline_coarse_classification():\n",
    "    pipe_for_text, pipe_for_agent = pipelines_for_text_and_agent()\n",
    "\n",
    "    feats = FeatureUnion([('text', pipe_for_text), \n",
    "                          ('agent', pipe_for_agent)],\n",
    "                        transformer_weights = { 'text': 1, 'agent': 2})\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', feats),\n",
    "        ('classifier', LinearSVC(loss='hinge'))\n",
    "    ])\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check for coarse classification\n",
    "\n",
    "A quick sanity check to review predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Question       0.80      0.91      0.85        58\n",
      "   Response       0.81      0.91      0.86        57\n",
      "  Statement       0.80      0.20      0.32        20\n",
      "\n",
      "avg / total       0.81      0.81      0.78       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_sets(independentvars = ['text', 'agent_type'], \n",
    "                                                       dependentVars = 'gold_label_simple')\n",
    "\n",
    "pipeline = pipeline_coarse_classification()\n",
    "pipeline.fit(X_train, y_train.values.ravel())\n",
    "preds = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n",
    "confidence = pipeline.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the pipeline for fine classification: depends on agent, text, and coarse gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_fine_classification():\n",
    "    pipe_for_text, pipe_for_agent = pipelines_for_text_and_agent()\n",
    "    \n",
    "    pipe_for_coarse_tag = Pipeline([\n",
    "                    ('selector', TextSelector(key='gold_label_simple')),\n",
    "                    ('cv', CountVectorizer())\n",
    "                ])   \n",
    "    \n",
    "\n",
    "    feats = FeatureUnion([('text', pipe_for_text), \n",
    "                          ('agent', pipe_for_agent),\n",
    "                          ('tag', pipe_for_coarse_tag)\n",
    "                         ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('features', feats),\n",
    "        ('classifier', LinearSVC(loss='hinge'),\n",
    "        )\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check for fine classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      " BinaryOpenQuestion       0.44      0.44      0.44        18\n",
      " BinaryOpenResponse       0.33      0.19      0.24        16\n",
      "     BinaryQuestion       0.11      0.12      0.12         8\n",
      "     BinaryResponse       0.22      0.71      0.33         7\n",
      "       Confirmation       0.38      0.60      0.46         5\n",
      "    ContextQuestion       0.35      0.26      0.30        27\n",
      "    ContextResponse       0.50      0.29      0.37        24\n",
      "MultiChoiceQuestion       0.00      0.00      0.00         0\n",
      "MultiChoiceResponse       0.00      0.00      0.00         0\n",
      "       OpenQuestion       0.43      0.60      0.50         5\n",
      "              Other       0.50      0.17      0.25         6\n",
      "          Statement       0.67      0.63      0.65        19\n",
      "\n",
      "        avg / total       0.42      0.37      0.38       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_sets(independentvars = ['text', 'agent_type', 'gold_label_simple'], \n",
    "                                                       dependentVars = 'gold_label_extended')\n",
    "\n",
    "pipeline = pipeline_fine_classification()\n",
    "pipeline.fit(X_train, y_train.values.ravel())\n",
    "preds = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "we will do a grid search; the parameter space is vast. For instructive purposes we shall use just the relative weights between the text feature space and the agent feature space \n",
    "\n",
    "(text weight, agent weight) = [ ((1, 1), (1, 2), .... (1, 4)), ((4, 1), (3, 1), ..]\n",
    "\n",
    "We can control the number of folds (number, size of the validation sets). Default = 4, at the user's discretion. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(pipeline, cv=4):\n",
    "    weights_range_1 = range(1, 5, 1)\n",
    "    weights_1 = [{ 'text': 1, 'agent': weight} for weight in weights_range_1]\n",
    "    weights_range_2 = range(2, 5, 1)\n",
    "    weights_2 = [{ 'text': weight, 'agent': 1} for weight in weights_range_2]\n",
    "    features__transformer_weights = []\n",
    "\n",
    "    features__transformer_weights.extend(weights_1)\n",
    "    features__transformer_weights.extend(weights_2)\n",
    "    \n",
    "    hyperparameters = {'features__transformer_weights': features__transformer_weights}\n",
    "    clf = GridSearchCV(pipeline, hyperparameters, cv=cv)\n",
    "\n",
    "    clf.fit(X_train, y_train.values.ravel())\n",
    "    return clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for coarse classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features__transformer_weights': {'text': 1, 'agent': 1}}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Question       0.80      0.91      0.85        58\n",
      "   Response       0.81      0.91      0.86        57\n",
      "  Statement       0.80      0.20      0.32        20\n",
      "\n",
      "avg / total       0.81      0.81      0.78       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_sets(independentvars = ['text', 'agent_type'], \n",
    "                                                       dependentVars = 'gold_label_simple')\n",
    "\n",
    "pipeline = pipeline_coarse_classification()\n",
    "clf = GridSearch(pipeline)\n",
    "\n",
    "clf.refit\n",
    "preds = clf.predict(X_test)\n",
    "print(clf.best_params_)\n",
    "df_test['predicted_label_simple'] = preds\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for fine classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'features__transformer_weights': {'text': 1, 'agent': 1}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      " BinaryOpenQuestion       0.44      0.44      0.44        18\n",
      " BinaryOpenResponse       0.33      0.19      0.24        16\n",
      "     BinaryQuestion       0.11      0.12      0.12         8\n",
      "     BinaryResponse       0.22      0.71      0.33         7\n",
      "       Confirmation       0.38      0.60      0.46         5\n",
      "    ContextQuestion       0.35      0.26      0.30        27\n",
      "    ContextResponse       0.50      0.29      0.37        24\n",
      "MultiChoiceQuestion       0.00      0.00      0.00         0\n",
      "MultiChoiceResponse       0.00      0.00      0.00         0\n",
      "       OpenQuestion       0.43      0.60      0.50         5\n",
      "              Other       0.50      0.17      0.25         6\n",
      "          Statement       0.67      0.63      0.65        19\n",
      "\n",
      "        avg / total       0.42      0.37      0.38       135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_train_test_sets(independentvars = ['text', 'agent_type', 'gold_label_simple'], \n",
    "                                                       dependentVars = 'gold_label_extended')\n",
    "\n",
    "pipeline = pipeline_fine_classification()\n",
    "clf = GridSearch(pipeline)\n",
    "\n",
    "clf.refit\n",
    "preds = clf.predict(X_test)\n",
    "print(clf.best_params_)\n",
    "df_test['predicted_label_extended'] = preds\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing into File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('interactions_test_predictions.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work\n",
    "\n",
    "What does not classify well?\n",
    "\n",
    "\n",
    "in coarse classification, the question and response categories are reasonably classified (f_score = 0.85), the recall of the statement category fails miserably - need to understand why is that. Observe the text for statements and try to find a bias.\n",
    "\n",
    "in fine classification, the results are admittedly poor.\n",
    "What we can do to improve? One glaring problem is obvious, I think: we have not taken into account the prior information that the fine category depends on the gold coarse category in a very strict way. I.e, at the moment, nothing bars our classifier from assigning, say, BinaryResponse as a fine tag given Statement as a coarse tag - a mistake. We must model this information - SVC will be inappropriate here; we have to cluster hierarchically. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
